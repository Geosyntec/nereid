# Austin Orr 11/27/2019
# This approach is the fastest to build and yields reasonably
# small images at ~500mb each. The alpine+install variant of this
# pattern is able to reduce each image to ~250MB but takes considerable
# time to build and is considerably more complex for scipy and pandas.

FROM node:lts-bullseye as frontend
WORKDIR /app
COPY ./nereid/static/frontend .
RUN npm install . && npm run build
CMD ["bash", "-c", "while true; do sleep 1; done"]


FROM python:3.11-bullseye as nereid_install
RUN apt-get update -y \
    && apt-get install -y --no-install-recommends graphviz \
    && rm -rf /var/lib/apt/lists/*
WORKDIR /nereid
CMD ["bash", "-c", "while true; do sleep 1; done"]


FROM redis:6.2.13-alpine3.18 as redis
COPY redis.conf /redis.conf
CMD ["redis-server", "/redis.conf"]


FROM python:3.11.7-alpine3.19 as flower
RUN apk add --no-cache ca-certificates tzdata && update-ca-certificates
RUN pip install --no-cache-dir redis==4.6.0 flower==1.0.0 celery==5.3.4
ENV PYTHONUNBUFFERED=1 PYTHONHASHSEED=random PYTHONDONTWRITEBYTECODE=1
ENV FLOWER_DATA_DIR=/data
ENV PYTHONPATH=${FLOWER_DATA_DIR}
WORKDIR $FLOWER_DATA_DIR
# Add a user with an explicit UID/GID and create necessary directories
ENV IMG_USER=flower
RUN set -eux; \
    addgroup -g 1000 ${IMG_USER}; \
    adduser -u 1000 -G ${IMG_USER} ${IMG_USER} -D; \
    mkdir -p "$FLOWER_DATA_DIR"; \
    chown ${IMG_USER}:${IMG_USER} "$FLOWER_DATA_DIR"
USER ${IMG_USER}
VOLUME $FLOWER_DATA_DIR
# Default port
EXPOSE 5555
CMD ["celery", "flower"]


FROM python:3.11.7-bullseye as builder
COPY requirements/requirements_dev.txt /requirements_dev.txt
COPY requirements/requirements_server.txt /requirements_server.txt
RUN mkdir /wheels && \
    pip wheel \
    --wheel-dir=/wheels \
    -r /requirements_dev.txt \
    -r /requirements_server.txt


FROM python:3.11.7-slim-bullseye as core-runtime
RUN apt-get update -y \
    && apt-get install -y --no-install-recommends graphviz \
    && rm -rf /var/lib/apt/lists/*
WORKDIR /nereid
COPY ./scripts /
RUN chmod +x /start.sh /start-reload.sh /prestart.sh /prestart-worker.sh /run-worker.sh
ENV PYTHONPATH=/nereid
ENV PATH=/opt/venv/bin:$PATH
COPY ./nereid /nereid/nereid
COPY --from=frontend /app/dist /nereid/nereid/static/frontend


# you are here, trying to get the requirements loaded, and rearranging the shared containers.
FROM python:3.11.7-slim-bullseye as core-env
COPY --from=builder /wheels /core
COPY requirements/requirements_worker.txt /requirements_worker.txt
RUN python -m venv /opt/venv
# Make sure we use the virtualenv:
ENV PATH=/opt/venv/bin:$PATH
RUN pip install \
    --no-index \
    --no-cache-dir \
    --find-links=/core \
    -r /requirements_worker.txt \
    && rm -rf /core/*


FROM core-runtime as celeryworker
# Add a user with an explicit UID/GID and create necessary directories
ENV IMG_USER=celeryworker
RUN addgroup --gid 1000 ${IMG_USER} \
    && adduser --system --disabled-password --uid 1000 --gid 1000 ${IMG_USER} \
    && chown -R ${IMG_USER}:${IMG_USER} /nereid
USER ${IMG_USER}
COPY --from=core-env --chown=${IMG_USER} /opt/venv /opt/venv
CMD /run-worker.sh


FROM core-env as server-env
COPY requirements/requirements_nereid.txt /requirements_nereid.txt
COPY requirements/requirements_server.txt /requirements_server.txt
COPY --from=builder /wheels /core
RUN python -m venv /opt/venv
# Make sure we use the virtualenv:
ENV PATH=/opt/venv/bin:$PATH
RUN pip install \
    --no-index \
    --no-cache-dir \
    --find-links=/core \
    -r /requirements_nereid.txt \
    -r /requirements_server.txt \
    && rm -rf /serve/*


FROM core-runtime as nereid
COPY --from=server-env /opt/venv /opt/venv
COPY gunicorn_conf.py /gunicorn_conf.py
EXPOSE 80
CMD /start.sh


FROM core-env as test-env
COPY requirements/requirements_dev.txt /requirements_dev.txt
COPY --from=builder /wheels /core
RUN python -m venv /opt/venv
# Make sure we use the virtualenv:
ENV PATH=/opt/venv/bin:$PATH
RUN pip install \
    --no-index \
    --no-cache-dir \
    --find-links=/core \
    -r /requirements_dev.txt \
    && rm -rf /tsts/*


FROM core-runtime as nereid-tests
COPY --from=test-env /opt/venv /opt/venv
COPY .coveragerc /nereid/.coveragerc
COPY conftest.py /nereid/conftest.py
## This will make the container wait, doing nothing, but alive
CMD ["bash", "-c", "while true; do sleep 1; done"]


FROM python:3.11-bullseye as nereid-edge
RUN apt-get update && apt-get install -y graphviz
COPY requirements/requirements_dev.txt /requirements_dev.txt
RUN awk -F"==" '{print $1}' /requirements_dev.txt > /requirements_edge.txt
RUN cat requirements_edge.txt
RUN pip install -r /requirements_edge.txt
COPY ./nereid /nereid/nereid
COPY --from=frontend /app/dist /nereid/nereid/static/frontend
COPY ./scripts /
RUN chmod +x /start.sh /start-reload.sh /prestart.sh /prestart-worker.sh /run-worker.sh
WORKDIR /nereid
ENV PYTHONPATH=/nereid


FROM nereid-edge as nereid-edge-tests
CMD ["bash", "-c", "while true; do sleep 1; done"]


FROM nereid-edge as celeryworker-edge
ENV C_FORCE_ROOT=1
CMD /run-worker.sh
